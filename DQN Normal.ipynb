{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r1SbGSl08dpV"
   },
   "source": [
    "# Implementation details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0aPsv_Uw8gnW"
   },
   "source": [
    "There are 5 modules of Deep Q learning-\n",
    "1. CNN for interacting with the environment\n",
    "2. Experience replay so that gradient descent converges rather than diverges. It basically provides a sense of direction to where to move for reducing the loss\n",
    "3. A module to integrate both of the above modules and building a linking peice to complete the algorithm\n",
    "4. testing with the OpenAI gym environment\n",
    "5. Preprocessing of image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qhN9HfBa8m8W"
   },
   "source": [
    "# import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b-c4ry1e8hw5"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "import tensorflow as tf\n",
    "from skimage.color import rgb2gray\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s60GirGLC2vZ"
   },
   "source": [
    "# Initializing Gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7MM5qEoK_elV"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"SpaceInvaders-v0\")\n",
    "env = env.unwrapped\n",
    "env.seed(1)\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mku2QVDzSotY"
   },
   "source": [
    "# Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "74I-fefRSpob"
   },
   "outputs": [],
   "source": [
    "action_number=env.action_space.n\n",
    "learning_rate=0.00025\n",
    "batch_size=64\n",
    "stack_size = 4\n",
    "pretrain_length = batch_size   \n",
    "memory_size = 1000000         \n",
    "total_episodes = 50\n",
    "gamma=0.9\n",
    "state_size = [110, 84, 4]  \n",
    "explore_start = 1.0            \n",
    "explore_stop = 0.01\n",
    "decay_rate = 0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gsLNN1h3ASUL"
   },
   "source": [
    "# Pre processing of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7HzU9oweAEaM"
   },
   "outputs": [],
   "source": [
    "def frame_preprocessing(image_frame):\n",
    "    cropped_frame = image_frame[8:-12,4:-12,:]\n",
    "    preprocessed_frame = resize(cropped_frame, [110,84])\n",
    "    old_img_float32 = np.float32(preprocessed_frame)\n",
    "    gray = cv2.cvtColor(old_img_float32, cv2.COLOR_RGB2GRAY)\n",
    "    normalized_frame = gray/255.0\n",
    "    return normalized_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl3EuAquEERE"
   },
   "source": [
    "# Experience replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vH5syw0iRZPn"
   },
   "source": [
    "## Stacking frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a1958wlWRYP7"
   },
   "outputs": [],
   "source": [
    "stacked_frames  =  deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    frame=frame_preprocessing(state)\n",
    "    if is_new_episode:\n",
    "        stacked_frames = deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zSvGn-U1DHrL"
   },
   "source": [
    "## Memory class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "paUaFalaC9iK"
   },
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lx1hqJEnRt0m"
   },
   "source": [
    "## Dealing with empty memory problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FbWWj0GdEhHc"
   },
   "outputs": [],
   "source": [
    "memory = Memory(max_size = memory_size)\n",
    "for i in range(pretrain_length):\n",
    "    if i == 0:\n",
    "        state = env.reset()\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    #possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "    action = random.randint(1,env.action_space.n)-1\n",
    "    #action = possible_actions[choice]\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    #env.render()\n",
    "    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "    if done:\n",
    "        next_state = np.zeros(state.shape)\n",
    "        possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "        action = possible_actions[action]\n",
    "        print(action.shape)\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        state = env.reset()\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    else:\n",
    "        possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "        action = possible_actions[action]\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ThREIxSFHcG"
   },
   "source": [
    "# Q Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P5by7mO9GNIE"
   },
   "source": [
    "The Q network architecture-\n",
    "1. 16 filters with 8 X 8 kernel and 4 strides with relu activation\n",
    "2. 32 filters with 4 X 4 kernel and 2 strides with relu activation\n",
    "3. 256 fully connected layer\n",
    "4. linear layer with number of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "dQGq6ksIE5KD",
    "outputId": "86599dbf-aead-4f9d-a3d6-c9662112b5a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sirzech/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/rmsprop.py:123: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "input_state=tf.compat.v1.placeholder(tf.float32, shape=[None, *state_size],name=\"input_state\")\n",
    "y_j=tf.compat.v1.placeholder(tf.float32, shape=[None],name=\"y_j\")\n",
    "action_space=tf.compat.v1.placeholder(tf.float32, shape=[None, action_number],name=\"action_space\")\n",
    "cnn_layer_1=tf.keras.layers.Conv2D(filters=32,kernel_size=(8,8),strides=(4,4),activation=\"elu\")(input_state)\n",
    "cnn_layer_2=tf.keras.layers.Conv2D(filters=64,kernel_size=(4,4),strides=(2,2),activation=\"elu\")(cnn_layer_1)\n",
    "cnn_layer_3=tf.keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(2,2),activation=\"elu\")(cnn_layer_2)\n",
    "flatten_layer=tf.keras.layers.Flatten()(cnn_layer_3)\n",
    "fully_connected_layer_1=tf.keras.layers.Dense(512,activation='relu',name=\"fully_connected_layer_1\")(flatten_layer)\n",
    "output_layer=tf.keras.layers.Dense(action_number,name=\"output_layer\")(fully_connected_layer_1)\n",
    "action_output=tf.keras.layers.Softmax(name=\"action_output\")(output_layer)\n",
    "Q_value=tf.math.reduce_sum(tf.math.multiply(output_layer, action_space))\n",
    "loss=tf.math.reduce_mean(tf.math.square(y_j-Q_value))\n",
    "training=tf.compat.v1.train.RMSPropOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ko5UiAbHVG04"
   },
   "source": [
    "# Training Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "colab_type": "code",
    "id": "9ny4l6-LMGCc",
    "outputId": "b6e5eb01-67d3-4bd1-c063-ee76147191fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: 85.0 Explore P: 0.9937\n",
      "Model Saved\n",
      "Episode: 1 Total reward: 65.0 Explore P: 0.9876\n",
      "Episode: 2 Total reward: 105.0 Explore P: 0.9815\n",
      "Episode: 3 Total reward: 240.0 Explore P: 0.9722\n",
      "Episode: 4 Total reward: 55.0 Explore P: 0.9664\n",
      "Episode: 5 Total reward: 105.0 Explore P: 0.9605\n",
      "Model Saved\n",
      "Episode: 6 Total reward: 65.0 Explore P: 0.9557\n",
      "Episode: 7 Total reward: 265.0 Explore P: 0.9451\n",
      "Episode: 8 Total reward: 125.0 Explore P: 0.9399\n",
      "Episode: 9 Total reward: 105.0 Explore P: 0.9350\n",
      "Episode: 10 Total reward: 155.0 Explore P: 0.9290\n",
      "Model Saved\n",
      "Episode: 11 Total reward: 35.0 Explore P: 0.9242\n",
      "Episode: 12 Total reward: 180.0 Explore P: 0.9178\n",
      "Episode: 13 Total reward: 110.0 Explore P: 0.9130\n",
      "Episode: 14 Total reward: 310.0 Explore P: 0.9031\n",
      "Episode: 15 Total reward: 35.0 Explore P: 0.8991\n",
      "Model Saved\n",
      "Episode: 16 Total reward: 75.0 Explore P: 0.8940\n",
      "Episode: 17 Total reward: 315.0 Explore P: 0.8848\n",
      "Episode: 18 Total reward: 210.0 Explore P: 0.8767\n",
      "Episode: 19 Total reward: 180.0 Explore P: 0.8702\n",
      "Episode: 20 Total reward: 145.0 Explore P: 0.8643\n",
      "Model Saved\n",
      "Episode: 21 Total reward: 125.0 Explore P: 0.8591\n",
      "Episode: 22 Total reward: 215.0 Explore P: 0.8532\n",
      "Episode: 23 Total reward: 110.0 Explore P: 0.8475\n",
      "Episode: 24 Total reward: 120.0 Explore P: 0.8413\n",
      "Episode: 25 Total reward: 135.0 Explore P: 0.8350\n",
      "Model Saved\n",
      "Episode: 26 Total reward: 155.0 Explore P: 0.8291\n",
      "Episode: 27 Total reward: 180.0 Explore P: 0.8232\n",
      "Episode: 28 Total reward: 75.0 Explore P: 0.8189\n",
      "Episode: 29 Total reward: 105.0 Explore P: 0.8158\n",
      "Episode: 30 Total reward: 155.0 Explore P: 0.8106\n",
      "Model Saved\n",
      "Episode: 31 Total reward: 0.0 Explore P: 0.8064\n"
     ]
    }
   ],
   "source": [
    "saver = tf.compat.v1.train.Saver()\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    decay_step = 0\n",
    "    for i in range(total_episodes):\n",
    "      episode_rewards = []\n",
    "      state = env.reset()\n",
    "#       env.render()\n",
    "      state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "      while True:\n",
    "        epsilon=random.random()\n",
    "        decay_step +=1\n",
    "        explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "        if (explore_probability > epsilon):\n",
    "          action = random.randint(1,env.action_space.n)-1\n",
    "          next_state, reward, done, _ = env.step(action)\n",
    "        else:\n",
    "          action_probability=sess.run(action_output,feed_dict={input_state:state.reshape((1, *state.shape))})\n",
    "          action = np.random.choice(range(action_probability.shape[1]), p=action_probability.ravel())\n",
    "          next_state, reward, done, _ = env.step(action)\n",
    "        episode_rewards.append(reward)\n",
    "        if done:\n",
    "          next_state = np.zeros(state.shape)\n",
    "          possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "          action = possible_actions[action]\n",
    "          memory.add((state, action, reward, next_state, done))\n",
    "          total_reward = np.sum(episode_rewards)\n",
    "          print('Episode: {}'.format(i),\n",
    "                                  'Total reward: {}'.format(total_reward),\n",
    "                                  'Explore P: {:.4f}'.format(explore_probability))\n",
    "          break\n",
    "        else:\n",
    "          next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "          possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "          action = possible_actions[action]\n",
    "          memory.add((state, action, reward, next_state, done))\n",
    "          state=next_state\n",
    "        batch = memory.sample(batch_size)\n",
    "        states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "        actions_mb = np.array([each[1] for each in batch])\n",
    "        rewards_mb = np.array([each[2] for each in batch]) \n",
    "        next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "        dones_mb = np.array([each[4] for each in batch])\n",
    "        target_Qs_batch = []\n",
    "        Qs_next_state = sess.run(output_layer, feed_dict = {input_state: next_states_mb})\n",
    "        for j in range(len(batch)):\n",
    "          terminal = dones_mb[i]\n",
    "          if terminal:\n",
    "            target_Qs_batch.append(rewards_mb[i])\n",
    "          else:\n",
    "            target_Qs_batch.append(rewards_mb[i]+(gamma*np.max(Qs_next_state[i])))\n",
    "        targets_mb = np.array([each for each in target_Qs_batch])\n",
    "        loss_value, _ = sess.run([loss, training],\n",
    "                           feed_dict={input_state: states_mb,\n",
    "                                      y_j: targets_mb, \n",
    "                                      action_space: actions_mb})\n",
    "      if i % 5 == 0:\n",
    "        save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "        print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcHumO9GuZ7M"
   },
   "source": [
    "# Testing of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SOnw6YeojM1P"
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    total_test_rewards = []\n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    \n",
    "    for episode in range(1):\n",
    "        total_rewards = 0\n",
    "        \n",
    "        state = env.reset()\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "        print(\"****************************************************\")\n",
    "        print(\"EPISODE \", episode)\n",
    "        \n",
    "        while True:\n",
    "            Qs = sess.run(action_output,feed_dict={input_state:state.reshape((1, *state.shape))})\n",
    "            action = np.random.choice(range(Qs.shape[1]), p=Qs.ravel())\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            env.render()\n",
    "            total_rewards += reward\n",
    "            if done:\n",
    "                print (\"Score\", total_rewards)\n",
    "                total_test_rewards.append(total_rewards)\n",
    "                break    \n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "            state = next_state\n",
    "            \n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Deep Q Network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
