{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "import tensorflow as tf\n",
    "from skimage.color import rgb2gray\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"SpaceInvaders-v0\")\n",
    "env = env.unwrapped\n",
    "env.seed(1)\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_number=env.action_space.n\n",
    "learning_rate=0.00025\n",
    "batch_size=64\n",
    "stack_size = 4\n",
    "pretrain_length = batch_size   \n",
    "memory_size = 1000000         \n",
    "total_episodes = 50\n",
    "gamma=0.9\n",
    "state_size = [110, 84, 4]  \n",
    "explore_start = 1.0            \n",
    "explore_stop = 0.01\n",
    "decay_rate = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_preprocessing(image_frame):\n",
    "    cropped_frame = image_frame[8:-12,4:-12,:]\n",
    "    preprocessed_frame = resize(cropped_frame, [110,84])\n",
    "    old_img_float32 = np.float32(preprocessed_frame)\n",
    "    gray = cv2.cvtColor(old_img_float32, cv2.COLOR_RGB2GRAY)\n",
    "    normalized_frame = gray/255.0\n",
    "    return normalized_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_frames  =  deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    frame=frame_preprocessing(state)\n",
    "    if is_new_episode:\n",
    "        stacked_frames = deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(max_size = memory_size)\n",
    "for i in range(pretrain_length):\n",
    "    if i == 0:\n",
    "        state = env.reset()\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    #possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "    action = random.randint(1,env.action_space.n)-1\n",
    "    #action = possible_actions[choice]\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    #env.render()\n",
    "    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "    if done:\n",
    "        next_state = np.zeros(state.shape)\n",
    "        possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "        action = possible_actions[action]\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        state = env.reset()\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    else:\n",
    "        possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "        action = possible_actions[action]\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "input_state=tf.compat.v1.placeholder(tf.float32, shape=[None, *state_size],name=\"input_state\")\n",
    "y_j=tf.compat.v1.placeholder(tf.float32, shape=[None],name=\"y_j\")\n",
    "action_space=tf.compat.v1.placeholder(tf.float32, shape=[None, action_number],name=\"action_space\")\n",
    "cnn_layer_1=tf.keras.layers.Conv2D(filters=32,kernel_size=(8,8),strides=(4,4),activation=\"elu\")(input_state)\n",
    "cnn_layer_2=tf.keras.layers.Conv2D(filters=64,kernel_size=(4,4),strides=(2,2),activation=\"elu\")(cnn_layer_1)\n",
    "cnn_layer_3=tf.keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(2,2),activation=\"elu\")(cnn_layer_2)\n",
    "flatten_layer=tf.keras.layers.Flatten()(cnn_layer_3)\n",
    "fully_connected_layer_1=tf.keras.layers.Dense(512,activation='relu',name=\"fully_connected_layer_1\")(flatten_layer)\n",
    "output_layer=tf.keras.layers.Dense(action_number,name=\"output_layer\")(fully_connected_layer_1)\n",
    "action_output=tf.keras.layers.Softmax(name=\"action_output\")(output_layer)\n",
    "Q_value=tf.math.reduce_sum(tf.math.multiply(output_layer, action_space))\n",
    "loss=tf.math.reduce_mean(tf.math.square(y_j-Q_value))\n",
    "training=tf.compat.v1.train.RMSPropOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.compat.v1.train.Saver()\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    decay_step = 0\n",
    "    for i in range(total_episodes):\n",
    "      episode_rewards = []\n",
    "      state = env.reset()\n",
    "#       env.render()\n",
    "      state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "      while True:\n",
    "        epsilon=random.random()\n",
    "        decay_step +=1\n",
    "        explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "        if (explore_probability > epsilon):\n",
    "          action = random.randint(1,env.action_space.n)-1\n",
    "          next_state, reward, done, _ = env.step(action)\n",
    "        else:\n",
    "          action_probability=sess.run(action_output,feed_dict={input_state:state.reshape((1, *state.shape))})\n",
    "          action = np.random.choice(range(action_probability.shape[1]), p=action_probability.ravel())\n",
    "          next_state, reward, done, _ = env.step(action)\n",
    "        episode_rewards.append(reward)\n",
    "        if done:\n",
    "          next_state = np.zeros(state.shape)\n",
    "          possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "          action = possible_actions[action]\n",
    "          memory.add((state, action, reward, next_state, done))\n",
    "          total_reward = np.sum(episode_rewards)\n",
    "          print('Episode: {}'.format(i),\n",
    "                                  'Total reward: {}'.format(total_reward),\n",
    "                                  'Explore P: {:.4f}'.format(explore_probability))\n",
    "          break\n",
    "        else:\n",
    "          next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "          possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "          action = possible_actions[action]\n",
    "          memory.add((state, action, reward, next_state, done))\n",
    "          state=next_state\n",
    "        batch = memory.sample(batch_size)\n",
    "        states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "        actions_mb = np.array([each[1] for each in batch])\n",
    "        rewards_mb = np.array([each[2] for each in batch]) \n",
    "        next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "        dones_mb = np.array([each[4] for each in batch])\n",
    "        target_Qs_batch = []\n",
    "        Qs_next_state = sess.run(output_layer, feed_dict = {input_state: next_states_mb})\n",
    "        for j in range(len(batch)):\n",
    "          terminal = dones_mb[i]\n",
    "          if terminal:\n",
    "            target_Qs_batch.append(rewards_mb[i])\n",
    "          else:\n",
    "            target_Qs_batch.append(rewards_mb[i]+(gamma*np.max(Qs_next_state[i])))\n",
    "        targets_mb = np.array([each for each in target_Qs_batch])\n",
    "        loss_value, _ = sess.run([loss, training],\n",
    "                           feed_dict={input_state: states_mb,\n",
    "                                      y_j: targets_mb, \n",
    "                                      action_space: actions_mb})\n",
    "      if i % 5 == 0:\n",
    "        save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "        print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    total_test_rewards = []\n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    \n",
    "    for episode in range(1):\n",
    "        total_rewards = 0\n",
    "        \n",
    "        state = env.reset()\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "        print(\"****************************************************\")\n",
    "        print(\"EPISODE \", episode)\n",
    "        \n",
    "        while True:\n",
    "            Qs = sess.run(action_output,feed_dict={input_state:state.reshape((1, *state.shape))})\n",
    "            action = np.random.choice(range(Qs.shape[1]), p=Qs.ravel())\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            env.render()\n",
    "            total_rewards += reward\n",
    "            if done:\n",
    "                print (\"Score\", total_rewards)\n",
    "                total_test_rewards.append(total_rewards)\n",
    "                break    \n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "            state = next_state\n",
    "            \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
